{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e47f28",
   "metadata": {},
   "source": [
    "### üí° Additional Reading Material\n",
    "\n",
    "For a deeper, visual intuition of how neural networks function and learn, the following guide is an excellent resource.\n",
    "[**Neural Networks by 3Blue1Brown**](https://www.3blue1brown.com/lessons/neural-networks)\n",
    "\n",
    "These other tutorials come from the Course Notebooks Repository:\n",
    "\n",
    "> In-depth: \n",
    "[**Introduction to Tensors in PyTorch**](https://github.com/CSCN8010/CSCN8010/blob/main/dl_class_notebooks/01A_intro_to_tensors_pytorch.ipynb)\n",
    "\n",
    "> In-depth: \n",
    "[**Introduction to Tensors in TernsorFlow**](https://github.com/CSCN8010/CSCN8010/blob/main/dl_class_notebooks/01B_intro_to_tensors_tensorflow.ipynb)  \n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3fb92",
   "metadata": {},
   "source": [
    "# üß† Multi-Layered Perceptrons (MLPs)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Part 1: What is a Perceptron?\n",
    "\n",
    "- A perceptron is the simplest kind of neural network: a single neuron with:\n",
    "  - Inputs: $x_1, x_2, ..., x_n$\n",
    "  - Weights: $w_1, w_2, ..., w_n$\n",
    "  - Bias: $b$\n",
    "  - Activation: $\\hat{y} = \\text{activation}(\\sum x_i w_i + b)$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{activation}(x \\cdot w + b)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c406be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.5028297305107117\n"
     ]
    }
   ],
   "source": [
    "# Import the neural network module from PyTorch, which contains the building \n",
    "# blocks for models, such as layers and activation functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Single Perceptron with 2 inputs\n",
    "# Define a simple neural network model using PyTorch's Sequential API\n",
    "# This model consists of a single linear layer followed by a sigmoid activation function\n",
    "model = nn.Sequential(\n",
    "\n",
    "    # Linear layer with 2 input features and 1 output feature\n",
    "    # It performs a linear transformation on the input data.\n",
    "    # Internally, this layer contains two weights (one for each input) and one bias, which are\n",
    "    # initialized with random values by PyTorch when the model is created\n",
    "    nn.Linear(2, 1),\n",
    "\n",
    "    # This is the second layer, an activation function. It takes the single value from the nn.Linear layer \n",
    "    # and applies the sigmoid formula to it, squashing the result to a value between 0 and 1.\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Sample input\n",
    "# Create a tensor with two features (inputs) to pass through the model\n",
    "# This tensor represents a single data point with two features, which is the expected input shape for\n",
    "# the model defined above\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "\n",
    "# This performs the \"feedforward\" pass, where the input x is sent through the linear layer and then \n",
    "# the sigmoid layer to produce the final output.\n",
    "output = model(x)\n",
    "\n",
    "# This extracts the single numerical value from the output tensor and prints it as a standard Python number.\n",
    "print(\"Output:\", output.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfad56",
   "metadata": {},
   "source": [
    "### Mathematical Explanation for the output (assuming = 0.5028297305107117)\n",
    "\n",
    "The calculation happens in two steps, corresponding to the two layers in `nn.Sequential`.\n",
    "\n",
    "#### Step 1: The Linear Layer\n",
    "The `nn.Linear` layer calculates the weighted sum of the inputs plus a bias. The general formula is:\n",
    "\n",
    "$$z = (x_1 \\cdot w_1) + (x_2 \\cdot w_2) + b$$\n",
    "\n",
    "Where:\n",
    "* $x_1, x_2$ are the inputs (`1.0` and `2.0`).\n",
    "* $w_1, w_2$ are the weights for each input, initialized randomly by PyTorch.\n",
    "* $b$ is the bias, also initialized randomly.\n",
    "\n",
    "#### Step 2: The Sigmoid Activation Layer\n",
    "The result of the linear layer, $z$, is then passed to the sigmoid function, which calculates the final output:\n",
    "\n",
    "$$\\text{Output} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "#### Reproducing Your Specific Output\n",
    "Since the weights ($w_1, w_2$) and bias ($b$) are initialized randomly, the output will be different each time the script is run. To get your **exact output** of `0.50282973`, the randomly generated weights and bias must have produced an intermediate value $z$ that satisfies the sigmoid equation.\n",
    "\n",
    "We can work backward from your output to find what $z$ must have been:\n",
    "\n",
    "1.  **Inverse Sigmoid**: The inverse of the sigmoid function is the logit function.\n",
    "    $$z = \\sigma^{-1}(\\text{Output}) = \\ln\\left(\\frac{\\text{Output}}{1 - \\text{Output}}\\right)$$\n",
    "2.  **Calculation**:\n",
    "    $$z = \\ln\\left(\\frac{0.50282973}{1 - 0.50282973}\\right) = \\ln(1.01138) \\approx 0.011318$$\n",
    "\n",
    "So, the linear layer's random weights and bias must have resulted in this equation:\n",
    "$$(1.0 \\cdot w_1) + (2.0 \\cdot w_2) + b = 0.011318$$\n",
    "\n",
    "For example, a plausible set of random values PyTorch might have generated to get this result is $w_1=0.35$, $w_2=-0.2$, and $b=0.061318$.\n",
    "\n",
    "* **Linear Calculation**: $z = (1.0 \\cdot 0.35) + (2.0 \\cdot -0.2) + 0.061318 = 0.35 - 0.40 + 0.061318 = \\mathbf{0.011318}$\n",
    "* **Sigmoid Calculation**: $\\text{Output} = \\frac{1}{1 + e^{-0.011318}} \\approx \\mathbf{0.50282973}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3667b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Part 2: What is an MLP?\n",
    "\n",
    "- MLP = Multi-Layer Perceptron = Neural Network with:\n",
    "  - Input layer\n",
    "  - One or more Hidden Layers\n",
    "  - Output Layer\n",
    "- Each layer contains **neurons**, each performing:\n",
    "  - Weighted sum\n",
    "  - Activation function\n",
    "\n",
    "üß© Why Use MLPs?\n",
    "- MLPs can learn **non-linear** boundaries\n",
    "- The more hidden layers, the **deeper** the network (but beware of overfitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa86925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\StudentWork\\Code\\CSCN8010\\MultiLayerPerceptrons\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              ‚îÇ            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              ‚îÇ            \u001b[38;5;34m24\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              ‚îÇ             \u001b[38;5;34m9\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> (132.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33\u001b[0m (132.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> (132.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33\u001b[0m (132.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code uses the Keras library from TensorFlow to build, define, and compile a simple two-layer neural network \n",
    "# designed for a binary classification task.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# This initializes an empty model to which layers will be added.\n",
    "model = Sequential()\n",
    "\n",
    "# This adds a dense (fully connected) layer with 8 neurons, using the ReLU activation function.\n",
    "# The input dimension is set to 2, meaning the model expects input data with two features.\n",
    "# Dense: This is a standard, fully-connected neural network layer.\n",
    "# 8: Specifies that this layer has 8 neurons (or units)\n",
    "# input_dim=2: This indicates that the input to this layer will have 2 features.\n",
    "# activation='relu': This specifies that the ReLU (Rectified Linear Unit) activation function\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))  # hidden layer\n",
    "\n",
    "# This adds the second and final layer, which is the output layer.\n",
    "# 1: This layer has a single neuron\n",
    "# activation='sigmoid': This specifies that the sigmoid activation function will be used, \n",
    "# which is common for binary classification tasks.\n",
    "model.add(Dense(1, activation='sigmoid'))            # output layer\n",
    "\n",
    "#  configures the model's learning process before training begins.\n",
    "# optimizer='adam': This sets the optimizer, the algorithm used to update the network's weights \n",
    "# based on the training data. Adam is a popular and effective general-purpose optimizer.\n",
    "# loss='binary_crossentropy': This sets the loss function, which measures the error between \n",
    "# the model's predictions and the true labels. binary_crossentropy is the standard loss function \n",
    "# for binary classification tasks.\n",
    "# metrics=['accuracy']: This specifies the metric to be monitored during training. accuracy will \n",
    "# calculate the percentage of predictions that were correct.\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# prints a concise summary of the network, including:\n",
    "# The layers and their order.\n",
    "# The output shape of each layer.\n",
    "# The number of trainable parameters (weights and biases) in each layer and in total.\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b1331",
   "metadata": {},
   "source": [
    "This output provides a layer-by-layer summary of the model's architecture and the number of parameters it contains. Here's a breakdown of what each part means.\n",
    "\n",
    "***\n",
    "### Output Shape\n",
    "The output shape column shows the dimensionality of the data after it passes through a layer.\n",
    "\n",
    "* **`(None, 8)`**: For the first `Dense` layer.\n",
    "    * The **`None`** is a placeholder for the **batch size**. It means the model can accept a variable number of input samples (e.g., 16, 32, 64) at a time.\n",
    "    * The **`8`** means that for each input sample, this layer produces an output vector of size 8, corresponding to the **8 neurons** in the layer.\n",
    "* **`(None, 1)`**: For the second `Dense` layer.\n",
    "    * The **`None`** is again the batch size.\n",
    "    * The **`1`** means the layer outputs a single value for each input sample, corresponding to the **1 neuron** in the output layer.\n",
    "\n",
    "***\n",
    "### Parameter Calculation (Param #)\n",
    "Parameters are the internal variables of the model that are learned during training; they consist of **weights** and **biases**. The formula for a `Dense` layer is:\n",
    "\n",
    "$$\\text{Parameters} = (\\text{input\\_features} \\times \\text{neurons}) + \\text{neurons}$$\n",
    "\n",
    "* **`dense_2` (24 params)**:\n",
    "    * This layer takes an input with **2 features** (`input_dim=2`) and has **8 neurons**.\n",
    "    * **Weights**: $2 \\text{ inputs} \\times 8 \\text{ neurons} = 16 \\text{ weights}$\n",
    "    * **Biases**: Each of the 8 neurons has 1 bias term, so there are $8 \\text{ biases}$.\n",
    "    * **Total**: $16 + 8 = 24$ parameters.\n",
    "\n",
    "* **`dense_3` (9 params)**:\n",
    "    * This layer's input is the output of the previous layer, which had **8 neurons**, so it takes **8 features**. This layer has **1 neuron**.\n",
    "    * **Weights**: $8 \\text{ inputs} \\times 1 \\text{ neuron} = 8 \\text{ weights}$\n",
    "    * **Biases**: The single neuron has 1 bias term, so there is $1 \\text{ bias}$.\n",
    "    * **Total**: $8 + 1 = 9$ parameters.\n",
    "\n",
    "***\n",
    "### Total, Trainable & Non-trainable Params\n",
    "\n",
    "* **Total params (33)**: This is the sum of the parameters from all layers in the model.\n",
    "    $$24 \\text{ (from dense\\_2)} + 9 \\text{ (from dense\\_3)} = 33$$\n",
    "* **Trainable params (33)**: These are the parameters that the optimizer will adjust during training to minimize the loss function. By default, all parameters in a `Dense` layer are trainable.\n",
    "* **Non-trainable params (0)**: These are parameters that are \"frozen\" and are not updated during training. This is often used in more advanced techniques like transfer learning. Since all layers in this model are set to be trainable by default, this count is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7d8bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Part 3: Activation Functions Explained\n",
    "\n",
    "| Function | Formula | Range | Use Case |\n",
    "|----------|---------|-------|----------|\n",
    "| **Sigmoid** | $\\sigma(x) = \\frac{1}{1+e^{-x}}$ | (0, 1) | Binary classification |\n",
    "| **ReLU** | $f(x) = \\max(0, x)$ | [0, ‚àû) | Most common hidden layer |\n",
    "| **Tanh** | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (‚àí1, 1) | When negative outputs help |\n",
    "\n",
    "üß† Tip: Use **ReLU** in hidden layers and **Sigmoid/Softmax** in output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc2f152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "relu = np.maximum(0, x)\n",
    "tanh = np.tanh(x)\n",
    "\n",
    "plt.plot(x, sigmoid, label='Sigmoid')\n",
    "plt.plot(x, relu, label='ReLU')\n",
    "plt.plot(x, tanh, label='Tanh')\n",
    "plt.legend(); plt.title(\"Activation Functions\"); plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefa1a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Part 4: How Feedforward Works\n",
    "\n",
    "1. Start with input vector $X$\n",
    "2. Compute weighted sum: $z = XW + b$\n",
    "3. Apply activation function to get output: $a = f(z)$\n",
    "4. Pass output to next layer (if any)\n",
    "5. Final output is prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133cef9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Part 5: Backpropagation in Detail\n",
    "\n",
    "1. **Forward pass**: Compute prediction $\\hat{y}$\n",
    "2. **Compute error**: $\\text{loss} = y - \\hat{y}$\n",
    "3. **Backpropagate error**: Chain rule to compute gradient\n",
    "4. **Update weights**:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot \\frac{\\partial \\text{loss}}{\\partial w}\n",
    "$$\n",
    "\n",
    "‚ö†Ô∏è Too high: overshooting  \n",
    "‚ö†Ô∏è Too low: slow convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fb24c",
   "metadata": {},
   "source": [
    "The code below implements the complete training process for a simple Multi-Layered Perceptron (MLP) using the backpropagation algorithm using PyTorch.\n",
    "\n",
    "The two key imports are:\n",
    "\n",
    "`from torch.optim import SGD` imports the **Stochastic Gradient Descent (SGD)** optimizer from PyTorch's optimization module, `torch.optim`. \n",
    "> We will explore it in more detail after the code run below.\n",
    "\n",
    "`torch.nn.functional` is a PyTorch module that provides a wide range of functions used for building neural network models, such as activation functions, loss functions, and other operations like pooling and normalization.\n",
    "\n",
    "Typically imported with the alias `F`, these are **stateless** functions, meaning they don't hold any learnable parameters like weights or biases.\n",
    "\n",
    "-----\n",
    "\n",
    "#### Key Distinction: Layers vs. Functions\n",
    "\n",
    "The primary difference in PyTorch is between stateful layers (`torch.nn.Module`) and stateless functions (`torch.nn.functional`).\n",
    "\n",
    "  * **`torch.nn` (Module-based layers)**: These are Python classes that hold **state**, most importantly, the **learnable parameters** (weights and biases). You define them in your model's `__init__` method.\n",
    "\n",
    "      * **Examples**: `nn.Linear`, `nn.Conv2d`, `nn.BatchNorm2d`.\n",
    "\n",
    "  * **`torch.nn.functional` (Functional API)**: These are simple functions that perform operations but have no learnable parameters of their own. You typically call them directly inside the `forward` method.\n",
    "\n",
    "      * **Examples**: `F.relu`, `F.sigmoid`, `F.max_pool2d`, `F.binary_cross_entropy`.\n",
    "\n",
    "#### When to Use Which\n",
    "\n",
    "The choice often comes down to whether the operation needs to store parameters.\n",
    "\n",
    "  * **Use `nn.Module` layers** for any operation with learnable weights, like linear transformations, convolutions, or batch normalization.\n",
    "  * **Use `nn.functional`** for operations that don't have learnable weights, like activation functions (ReLU, sigmoid), pooling, or dropout. This is often a stylistic choice, as PyTorch also provides module-based versions (e.g., `nn.ReLU()`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa35a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Output:\n",
      " tensor([[0.0297],\n",
      "        [0.9958],\n",
      "        [0.9958],\n",
      "        [0.0027]])\n"
     ]
    }
   ],
   "source": [
    "# This code defines a simple Multi-Layered Perceptron (MLP) using PyTorch,\n",
    "# which consists of two fully connected layers with a ReLU activation function in between.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the first fully connected layer with 2 input features and 8 output features\n",
    "        self.fc1 = nn.Linear(2, 8)\n",
    "        # Define the second fully connected layer with 8 input features and 1 output feature\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "\n",
    "    # The forward method defines the forward pass of the network\n",
    "    # It takes an input tensor x, applies the first layer followed by ReLU activation,\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Then applies the second layer followed by a sigmoid activation function\n",
    "        # The sigmoid function squashes the output to a range between 0 and 1,\n",
    "        return torch.sigmoid(self.fc2(x))\n",
    "\n",
    "# Dummy dataset (XOR-like)\n",
    "X = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
    "y = torch.tensor([[0.],[1.],[1.],[0.]])\n",
    "\n",
    "model = SimpleMLP()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):\n",
    "    y_pred = model(X)\n",
    "    loss = F.binary_cross_entropy(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Trained Output:\\n\", model(X).detach())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e0d79",
   "metadata": {},
   "source": [
    "This output shows your model's final predictions for each of the four data points in your input tensor `X` after the training process is complete. Since the model's final layer is a `sigmoid` function, each output value is a probability between 0 and 1.\n",
    "\n",
    "The model has successfully learned the non-linear XOR pattern from the data.\n",
    "\n",
    "---\n",
    "#### Output Breakdown\n",
    "\n",
    "Here's a comparison of the model's predictions to the target labels (`y`):\n",
    "\n",
    "* **Input `[0,0]`** ‚Üí **Prediction ($\\hat{y}$) = `0.0297`**\n",
    "    * The target ($y$) was `0`. The model's output is very close to 0, indicating a correct prediction.\n",
    "\n",
    "* **Input `[0,1]`** ‚Üí **Prediction ($\\hat{y}$) = `0.9958`**\n",
    "    * The target ($y$) was `1`. The model's output is very close to 1, indicating a correct prediction.\n",
    "\n",
    "* **Input `[1,0]`** ‚Üí **Prediction ($\\hat{y}$) = `0.9958`**\n",
    "    * The target ($y$) was `1`. The model's output is also very close to 1, a correct prediction.\n",
    "\n",
    "* **Input `[1,1]`** ‚Üí **Prediction ($\\hat{y}$) = `0.0027`**\n",
    "    * The target ($y$) was `0`. The model's output is very close to 0, indicating another correct prediction.\n",
    "\n",
    "In essence, the 2,000 training loops successfully adjusted the model's weights and biases so that it can now accurately distinguish between the different input patterns and produce the correct output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155cdf90",
   "metadata": {},
   "source": [
    "#### What is an Optimizer?\n",
    "\n",
    "In neural networks, an **optimizer** is an algorithm that adjusts the model's internal parameters (its weights and biases) to minimize the error, or **loss**. It's the engine that drives the learning process.\n",
    "\n",
    "#### What is SGD?\n",
    "\n",
    "**Stochastic Gradient Descent** is one of the most fundamental optimization algorithms. Here‚Äôs what the name means:\n",
    "\n",
    "  * **Gradient Descent**: Imagine the model's loss as a hilly landscape. The goal is to find the lowest point (minimum loss). Gradient descent does this by calculating the slope (gradient) at the model's current position and taking a step downhill. It repeats this process until it settles at the bottom of a valley. ‚õ∞Ô∏è\n",
    "  * **Stochastic**: Instead of using the entire dataset to calculate the slope for each step (which would be very slow), SGD uses a small, random subset of the data called a **mini-batch**. This makes the training process much faster and can help the model avoid getting stuck in poor solutions.\n",
    "\n",
    "#### How It's Used in Code\n",
    "\n",
    "You first instantiate the `SGD` optimizer, telling it which model parameters to update and setting the **learning rate (`lr`)**, which controls the size of each \"step\" it takes.\n",
    "\n",
    "```python\n",
    "# SGD needs the model's parameters to know what to update\n",
    "# 'lr' is the learning rate, a critical hyperparameter\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Inside the training loop...\n",
    "# After calculating gradients with loss.backward()...\n",
    "optimizer.step() # This line updates the model's weights and biases\n",
    "```\n",
    "\n",
    "In short, `from torch.optim import SGD` gives you a powerful tool to actually train your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c529622",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Part 6: Hyperparameters in MLPs\n",
    "\n",
    "| Hyperparameter | Description |\n",
    "|----------------|-------------|\n",
    "| Learning Rate  | Step size for weight update |\n",
    "| Epochs         | Full dataset passes |\n",
    "| Batch Size     | Subset used per gradient update |\n",
    "| Optimizer      | Algorithm to minimize error (e.g., SGD, Adam) |\n",
    "| Loss Function  | Measures how far prediction is from truth |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc8bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4572 - loss: 0.7586   \n",
      "Epoch 2/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4973 - loss: 0.6941 \n",
      "Epoch 3/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7050 - loss: 0.6284 \n",
      "Epoch 4/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.8382 - loss: 0.5710\n",
      "Epoch 5/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - accuracy: 0.8631 - loss: 0.5081\n",
      "Epoch 6/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - accuracy: 0.8756 - loss: 0.4492\n",
      "Epoch 7/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8669 - loss: 0.4151 \n",
      "Epoch 8/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8634 - loss: 0.3900 \n",
      "Epoch 9/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8743 - loss: 0.3494 \n",
      "Epoch 10/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8741 - loss: 0.3415 \n",
      "Epoch 11/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8636 - loss: 0.3318 \n",
      "Epoch 12/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8793 - loss: 0.3069 \n",
      "Epoch 13/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8726 - loss: 0.3051 \n",
      "Epoch 14/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8760 - loss: 0.3045 \n",
      "Epoch 15/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - accuracy: 0.8857 - loss: 0.2908\n",
      "Epoch 16/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8862 - loss: 0.2812 \n",
      "Epoch 17/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8713 - loss: 0.3067 \n",
      "Epoch 18/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step - accuracy: 0.8928 - loss: 0.2784\n",
      "Epoch 19/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8880 - loss: 0.2682 \n",
      "Epoch 20/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8823 - loss: 0.2720 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x212dae165d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0997d",
   "metadata": {},
   "source": [
    "Here is a detailed explanation of Learning Rate and Batch Size, two of the most important hyperparameters you'll tune before training a neural network.\n",
    "\n",
    "---\n",
    "#### 1. Learning Rate\n",
    "The **learning rate**, often denoted as `lr` in code, is a hyperparameter that controls how much the model's weights are adjusted with respect to the loss gradient during training. Think of it as the **size of the steps** the optimizer takes to reach the lowest point in the loss landscape.\n",
    "\n",
    "* **How it Works**: The learning rate is a small positive number (e.g., `0.1`, `0.01`, `0.001`) that scales the gradient before it's used to update the weights. The update rule is conceptually:\n",
    "    $$\\text{new\\_weight} = \\text{old\\_weight} - (\\text{learning\\_rate} \\times \\text{gradient})$$\n",
    "\n",
    "* **Impact of the Learning Rate**:\n",
    "    * **A high learning rate** causes the model to take large steps. This can speed up learning, but it risks **overshooting** the lowest point of the loss function. The model might fail to converge, with the loss bouncing around erratically or even increasing.\n",
    "    * **A low learning rate** causes the model to take tiny, careful steps. While this is more likely to lead to a good solution, it can make training extremely slow. It also runs the risk of getting stuck in a suboptimal \"local minimum.\"\n",
    "\n",
    "Finding the right learning rate is critical for efficient training and is often one of the first hyperparameters to tune.\n",
    "\n",
    "---\n",
    "#### 2. Batch Size\n",
    "The **batch size** defines the number of training samples to work through before the model‚Äôs internal parameters are updated. In other words, it's how many examples the model sees before it performs a backpropagation step.\n",
    "\n",
    "* **How it Works**: Instead of calculating the error gradient from the entire dataset at once, the data is broken into smaller \"mini-batches.\" The model performs a forward pass, a backward pass, and a weight update for each mini-batch.\n",
    "\n",
    "* **Impact of the Batch Size**:\n",
    "    * **A small batch size** (e.g., 1, which is true Stochastic Gradient Descent) provides a \"noisier\" but faster estimate of the gradient. The training process can be unstable, with the loss fluctuating a lot, but the randomness can sometimes help the model find better solutions.\n",
    "    * **A large batch size** provides a more accurate and stable gradient estimate, leading to a smoother convergence. However, it requires more memory and computational power per step.\n",
    "    * **A full batch size** (where the batch includes the entire dataset, known as Batch Gradient Descent) is very computationally expensive and is rarely used in modern deep learning.\n",
    "\n",
    "The most common approach is **mini-batch gradient descent**, which uses a batch size between 1 and the full dataset size (typically powers of 2, like 32, 64, or 128), offering a balance between computational efficiency and training stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642965ce",
   "metadata": {},
   "source": [
    "The **loss** and **accuracy** values in your output are key indicators of how well your model is learning during the training process. They are calculated for each epoch based on the functions you specified in `model.compile()`.\n",
    "\n",
    "---\n",
    "#### 3. Accuracy\n",
    "\n",
    "**Accuracy** is a more intuitive metric that measures the fraction of predictions your model got right.\n",
    "\n",
    "* **How it works**: For each data point, the model's output (a probability) is converted into a definite class prediction. The standard threshold is 0.5.\n",
    "    * If the predicted probability is > 0.5, the model predicts class `1`.\n",
    "    * If the predicted probability is ‚â§ 0.5, the model predicts class `0`.\n",
    "    * This final prediction is then compared to the true label.\n",
    "\n",
    "* **The Formula**: Accuracy is calculated simply as:\n",
    "    $$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Samples}}$$\n",
    "\n",
    "The `accuracy` value you see (`0.4572` or 45.7%, `0.4973` or 49.7%, etc.) shows the percentage of the training samples the model classified correctly in that epoch. You can see this value increasing over time, which shows the model is learning.\n",
    "\n",
    "---\n",
    "#### 4. Loss Calculation (Cross Entropy)\n",
    "\n",
    "The **loss** (or error) is a measure of how wrong the model's predictions are compared to the true labels. The optimizer's job is to minimize this value.\n",
    "\n",
    "* **How it works**: You used `'binary_crossentropy'`, which is the standard loss function for two-class (binary) classification problems. It works by heavily penalizing the model when it makes a confident but incorrect prediction.\n",
    "    * If the true label is `1` and the model predicts a low probability like `0.05`, the loss will be very high.\n",
    "    * If the true label is `1` and the model predicts a high probability like `0.95`, the loss will be very low.\n",
    "\n",
    "* **The Formula**: For a single data point, the binary cross-entropy loss is calculated as:\n",
    "    $$L = -[y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y})]$$\n",
    "    * $y$ is the true label (0 or 1).\n",
    "    * $\\hat{y}$ is the model's predicted probability (a value between 0 and 1 from the sigmoid function).\n",
    "\n",
    "The `loss` value you see in the output (`0.7586`, `0.6941`, etc.) is the **average loss** across all samples processed in that epoch. **The goal of training is to make this number as small as possible**.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
