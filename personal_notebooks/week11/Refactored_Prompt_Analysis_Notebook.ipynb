{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2062d3",
   "metadata": {},
   "source": [
    "# üîß Refactored Notebook: Prompt Analysis for Energy Prediction using Deep Learning\n",
    "\n",
    "This notebook extends the chatbot prototype with NLP tools for analyzing a prompt and extracting features relevant to energy prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0035b7",
   "metadata": {},
   "source": [
    "### üìò Curriculum Learning 1.1: Token Count as Proxy for Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397cc862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to c:\\StudentWork\\Code\\CSCN8010\\\n",
      "[nltk_data]     Sustainable_AI_Project_Protocol\\Phase One\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to c:\\StudentWork\\Code\\CSCN8\n",
      "[nltk_data]     010\\Sustainable_AI_Project_Protocol\\Phase\n",
      "[nltk_data]     One\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt_tab' tokenizer...\n",
      "NLTK Data Paths: ['C:\\\\Users\\\\Eespinosa/nltk_data', 'c:\\\\StudentWork\\\\Code\\\\CSCN8010\\\\Sustainable_AI_Project_Protocol\\\\.venv\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\CSCN8010\\\\Sustainable_AI_Project_Protocol\\\\.venv\\\\share\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\CSCN8010\\\\Sustainable_AI_Project_Protocol\\\\.venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Eespinosa\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\CSCN8010\\\\Sustainable_AI_Project_Protocol\\\\Phase One\\\\nltk_data']\n",
      "Contents of nltk_data: ['tokenizers']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 1. Download NLTK data\n",
    "# Ensure 'punkt' is available and nltk_data path is set\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "print(\"Downloading 'punkt' tokenizer...\")\n",
    "nltk.download('punkt', download_dir=nltk_data_path, force=True)\n",
    "print(\"Downloading 'punkt_tab' tokenizer...\")\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# Always append the custom nltk_data path (if not already present)\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Debugging paths and contents\n",
    "print(\"NLTK Data Paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03960b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 11\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(prompt):\n",
    "    return len(word_tokenize(prompt.lower()))\n",
    "\n",
    "prompt = \"How can I reduce my energy cost when using LLMs?\"\n",
    "print(\"Token count:\", count_tokens(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee5f800",
   "metadata": {},
   "source": [
    "### üîó Curriculum Learning 2.1: Chain Rule-Based Scoring with N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35d60844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "# Generate a large synthetic token corpus with over 1000 domain-specific words\n",
    "# Covering automotive, food production, oil, electronics, agriculture, medical equipment, hygiene products\n",
    "\n",
    "import random\n",
    "\n",
    "# Word banks per domain\n",
    "domains = {\n",
    "    \"automotive\": [\n",
    "        \"engine\", \"transmission\", \"assembly\", \"OEM\", \"dealership\", \"tire\", \"brake\", \"dashboard\", \"fuel\", \"ignition\",\n",
    "        \"torque\", \"suspension\", \"steering\", \"battery\", \"chassis\", \"gearbox\", \"clutch\", \"radiator\", \"axle\", \"bumper\"\n",
    "    ],\n",
    "    \"food_production\": [\n",
    "        \"harvest\", \"fertilizer\", \"pesticide\", \"organic\", \"irrigation\", \"yield\", \"poultry\", \"dairy\", \"grain\", \"corn\",\n",
    "        \"packaging\", \"labeling\", \"nutrition\", \"distribution\", \"shelf_life\", \"inspection\", \"processing\", \"warehouse\", \"frozen\", \"perishable\"\n",
    "    ],\n",
    "    \"oil_gas\": [\n",
    "        \"refinery\", \"barrel\", \"pipeline\", \"petroleum\", \"diesel\", \"gasoline\", \"offshore\", \"drilling\", \"rig\", \"lubricant\",\n",
    "        \"exploration\", \"upstream\", \"downstream\", \"crude\", \"hydrocarbon\", \"fracturing\", \"compressor\", \"reservoir\", \"seismic\", \"platform\"\n",
    "    ],\n",
    "    \"electronics\": [\n",
    "        \"semiconductor\", \"resistor\", \"capacitor\", \"PCB\", \"microchip\", \"soldering\", \"transistor\", \"diode\", \"oscilloscope\", \"breadboard\",\n",
    "        \"LED\", \"IC\", \"voltage\", \"current\", \"amplifier\", \"microcontroller\", \"sensor\", \"IoT\", \"firmware\", \"FPGA\"\n",
    "    ],\n",
    "    \"agriculture\": [\n",
    "        \"tractor\", \"crop\", \"cultivation\", \"acreage\", \"cooperative\", \"greenhouse\", \"farm\", \"livestock\", \"milking\", \"plowing\",\n",
    "        \"seeding\", \"spraying\", \"harvesting\", \"rotation\", \"fertility\", \"tillage\", \"yield\", \"grains\", \"paddock\", \"pasture\"\n",
    "    ],\n",
    "    \"medical\": [\n",
    "        \"stethoscope\", \"thermometer\", \"syringe\", \"scalpel\", \"respirator\", \"defibrillator\", \"catheter\", \"IV\", \"diagnosis\", \"radiology\",\n",
    "        \"sterile\", \"disinfectant\", \"bandage\", \"infusion\", \"gown\", \"PPE\", \"intubation\", \"oxygen\", \"nebulizer\", \"vaccine\"\n",
    "    ],\n",
    "    \"hygiene\": [\n",
    "        \"soap\", \"sanitizer\", \"toilet_paper\", \"shampoo\", \"conditioner\", \"toothbrush\", \"toothpaste\", \"detergent\", \"cleanser\", \"disinfectant\",\n",
    "        \"deodorant\", \"napkin\", \"razor\", \"floss\", \"moisturizer\", \"lotion\", \"wipe\", \"spray\", \"cotton\", \"absorbent\"\n",
    "    ],\n",
    "    \"logistics\": [\n",
    "        \"shipping\", \"freight\", \"customs\", \"tariff\", \"inventory\", \"procurement\", \"tracking\", \"dispatch\", \"container\", \"pallet\",\n",
    "        \"warehouse\", \"fulfillment\", \"courier\", \"manifest\", \"cross_docking\", \"handover\", \"last_mile\", \"airwaybill\", \"consignment\", \"bill_of_lading\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Multiply each domain by 6 to exceed 1000 tokens and shuffle\n",
    "token_corpus = []\n",
    "for domain_words in domains.values():\n",
    "    token_corpus.extend(domain_words * 6)\n",
    "\n",
    "# Shuffle for realism\n",
    "random.shuffle(token_corpus)\n",
    "\n",
    "# Truncate if needed\n",
    "token_corpus = token_corpus[:1050]\n",
    "\n",
    "# Output the Python code version of this\n",
    "token_list_code = \"tokens = [\\n\" + \",\\n\".join(f'    \"{word}\"' for word in token_corpus) + \"\\n]\"\n",
    "token_list_code[:1000]  # preview only first 1000 characters\n",
    "\n",
    "unigrams = Counter(tokens)\n",
    "bigrams_list = list(bigrams(tokens))\n",
    "bigram_counts = Counter(bigrams_list)\n",
    "\n",
    "def bigram_probability(prompt):\n",
    "    words = word_tokenize(prompt.lower())\n",
    "    score = 1.0\n",
    "    for i in range(1, len(words)):\n",
    "        prev, curr = words[i - 1], words[i]\n",
    "        score *= bigram_counts[(prev, curr)] / unigrams[prev] if unigrams[prev] else 1e-6\n",
    "    return score\n",
    "\n",
    "print(\"Bigram Score:\", bigram_probability(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3a7d1",
   "metadata": {},
   "source": [
    "#### üìò Embedding Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f0124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Get training corpus from the preious section\n",
    "corpus = token_corpus\n",
    "model = Word2Vec(sentences=corpus, vector_size=50, window=2, min_count=1, sg=1, seed=42)\n",
    "\n",
    "def sentence_vector(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375021e9",
   "metadata": {},
   "source": [
    "### üìä Curriculum Learning 3.1: Combined Prompt Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5709ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_count': 100, 'avg_embedding_norm': 9.92105490783582, 'bigram_likelihood': 0.034}\n"
     ]
    }
   ],
   "source": [
    "def extract_prompt_features(prompt):\n",
    "    tokens = word_tokenize(prompt.lower())\n",
    "    token_count = len(tokens)\n",
    "    avg_vec = sentence_vector(prompt)\n",
    "    avg_norm = np.linalg.norm(avg_vec)\n",
    "    bigram_score = bigram_probability(prompt)\n",
    "    return {\n",
    "        \"token_count\": token_count,\n",
    "        \"avg_embedding_norm\": avg_norm,\n",
    "        \"bigram_likelihood\": bigram_score\n",
    "    }\n",
    "\n",
    "# Example usage using the provided long prompt\n",
    "prompt = \"Provide a comprehensive report detailing the environmental, economic, and geopolitical impact of offshore drilling in the Gulf of Mexico. Include historical data on oil spills, current safety regulations, the role of independent contractors, transportation logistics to refineries in Texas and Louisiana, and how recent changes in U.S. trade policy affect crude exports to Canada and Mexico. Also assess fluctuations in barrel pricing, regional employment statistics, and supply chain vulnerabilities related to pipeline maintenance and hurricane disruptions. Finally, compare with alternative extraction methods like fracking and tar sands.\"\n",
    "\n",
    "features = extract_prompt_features(prompt)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db5b94f",
   "metadata": {},
   "source": [
    "#### üßÆ Prompt Feature Extraction + Model Metadata + Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07257b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt =  Provide a comprehensive report detailing the environmental, economic, and geopolitical impact of offshore drilling in the Gulf of Mexico. Include historical data on oil spills, current safety regulations, the role of independent contractors, transportation logistics to refineries in Texas and Louisiana, and how recent changes in U.S. trade policy affect crude exports to Canada and Mexico. Also assess fluctuations in barrel pricing, regional employment statistics, and supply chain vulnerabilities related to pipeline maintenance and hurricane disruptions. Finally, compare with alternative extraction methods like fracking and tar sands.\n",
      "‚úÖ Normalized input features ready for model:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "layers",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "training_time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flops_per_hour",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "token_count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_embedding_norm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bigram_likelihood",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "491e8205-936c-4350-a919-7dbfae9edc43",
       "rows": [
        [
         "0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layers</th>\n",
       "      <th>training_time</th>\n",
       "      <th>flops_per_hour</th>\n",
       "      <th>token_count</th>\n",
       "      <th>avg_embedding_norm</th>\n",
       "      <th>bigram_likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   layers  training_time  flops_per_hour  token_count  avg_embedding_norm  \\\n",
       "0     0.0            0.0             0.0          0.0                 0.0   \n",
       "\n",
       "   bigram_likelihood  \n",
       "0                0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "\n",
    "# üîß Placeholder functions\n",
    "def sentence_vector(prompt):\n",
    "    return np.random.rand(300)\n",
    "\n",
    "def bigram_probability(prompt):\n",
    "    return 0.034\n",
    "\n",
    "# üß† Simple tokenizer (fallback)\n",
    "def simple_tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# üîç Feature extraction function\n",
    "def extract_prompt_features(prompt):\n",
    "    tokens = simple_tokenize(prompt)\n",
    "    token_count = len(tokens)\n",
    "    avg_vec = sentence_vector(prompt)\n",
    "    avg_norm = np.linalg.norm(avg_vec)\n",
    "    bigram_score = bigram_probability(prompt)\n",
    "    return {\n",
    "        \"token_count\": token_count,\n",
    "        \"avg_embedding_norm\": avg_norm,\n",
    "        \"bigram_likelihood\": bigram_score\n",
    "    }\n",
    "\n",
    "# ‚úèÔ∏è User prompt from the previous section\n",
    "print(f\"Prompt = \",prompt)\n",
    "\n",
    "# ‚öôÔ∏è LLM + server metadata\n",
    "llm_metadata = {\n",
    "    \"layers\": 12,\n",
    "    \"training_time\": 48,\n",
    "    \"flops_per_hour\": 2e12\n",
    "}\n",
    "\n",
    "# üì¶ Combine prompt + metadata\n",
    "prompt_features = extract_prompt_features(prompt)\n",
    "all_features = {**llm_metadata, **prompt_features}\n",
    "\n",
    "# üìä DataFrame and scaling\n",
    "df = pd.DataFrame([all_features])\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=df.columns)\n",
    "\n",
    "print(\"‚úÖ Normalized input features ready for model:\")\n",
    "display(scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639316d",
   "metadata": {},
   "source": [
    "### üìä Curriculum Learning (8): Multi-Layer Perceptron (MLP) Architecture\n",
    "\n",
    "Prepare features from the user prompt using classic NLP tools. These features will feed into a lightweight neural network to predict the energy cost of remote LLM inference.\n",
    "\n",
    "Note that the model is currently performing a forward pass using randomly initialized weights, without any training. That‚Äôs why you're seeing output, but it‚Äôs not meaningful yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beccdb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîã Estimated Energy Consumption: -0.5921454429626465 kWh\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# üìä DataFrame and scaling\n",
    "# This dictionary should come from your Prompt + Model metadata extractor\n",
    "all_features = {\n",
    "    \"layers\": 12,\n",
    "    \"training_time\": 48,\n",
    "    \"flops_per_hour\": 2e12,\n",
    "    \"token_count\": 105,\n",
    "    \"avg_embedding_norm\": 0.87,\n",
    "    \"bigram_likelihood\": 0.031\n",
    "}\n",
    "df = pd.DataFrame([all_features])\n",
    "\n",
    "# üß™ Normalize features\n",
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_array, columns=df.columns)\n",
    "\n",
    "# üß† Define MLP Model\n",
    "class EnergyEstimatorMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(EnergyEstimatorMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(8, 1)  # Output: Energy prediction (kWh)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# üßÆ Prepare input and model\n",
    "input_tensor = torch.tensor(scaled_array, dtype=torch.float32)\n",
    "input_dim = input_tensor.shape[1]\n",
    "mlp_model = EnergyEstimatorMLP(input_dim)\n",
    "\n",
    "# üîÆ Forward pass to get energy estimate\n",
    "with torch.no_grad():\n",
    "    prediction = mlp_model(input_tensor)\n",
    "    predicted_energy_kwh = prediction.item()\n",
    "\n",
    "print(\"üîã Estimated Energy Consumption:\", predicted_energy_kwh, \"kWh\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc029ed",
   "metadata": {},
   "source": [
    "‚úÖ Let's Add Training Capability\n",
    "Below is a minimal training pipeline for the EnergyEstimatorMLP using synthetic data. You can later replace this with real, labeled energy usage data.\n",
    "\n",
    "üîÅ Training Code: Curriculum Learning (8.1) ‚Äì MLP Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2846a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 23924.6406\n",
      "Epoch 10, Loss: 23916.6367\n",
      "Epoch 20, Loss: 23909.8145\n",
      "Epoch 30, Loss: 23904.6777\n",
      "Epoch 40, Loss: 23899.2734\n",
      "Epoch 50, Loss: 23894.3203\n",
      "Epoch 60, Loss: 23887.1348\n",
      "Epoch 70, Loss: 23881.5098\n",
      "Epoch 80, Loss: 23873.3496\n",
      "Epoch 90, Loss: 23865.5820\n",
      "Model trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# üîÅ Simulated training data (replace with real data when available)\n",
    "# Each row corresponds to scaled features: [layers, training_time, flops/hour, token_count, avg_embedding_norm, bigram_likelihood]\n",
    "X_train = torch.tensor(np.tile(scaled_array, (100, 1)), dtype=torch.float32)\n",
    "\n",
    "# Simulated target energy consumption values (in kWh)\n",
    "y_train = torch.tensor(np.random.normal(loc=150.0, scale=30.0, size=(100, 1)), dtype=torch.float32)\n",
    "\n",
    "# üìê Initialize model\n",
    "input_dim = X_train.shape[1]\n",
    "model = EnergyEstimatorMLP(input_dim)\n",
    "\n",
    "# ‚öôÔ∏è Loss and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# üìä Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(X_train)\n",
    "    loss = loss_fn(predictions, y_train)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ‚úÖ Save trained model (optional)\n",
    "torch.save(model.state_dict(), \"mlp_energy_estimator.pth\")\n",
    "print(\"Model trained and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a074a",
   "metadata": {},
   "source": [
    "Here are **5 practical actions** to help replace the synthetic data with **real, labeled energy usage data** and repeat the training of the `EnergyEstimatorMLP` model.\n",
    "\n",
    "\n",
    "## ‚úÖ Project-Based Actions\n",
    "\n",
    "### üîç 1. **Gather Prompt Logs and Metadata from an API Call**\n",
    "\n",
    "* **Task**: Use OpenAI or Hugging Face APIs to send prompts and collect metadata.\n",
    "* **Output**: For each prompt, record:\n",
    "\n",
    "  * Prompt text\n",
    "  * Number of tokens\n",
    "  * Model used\n",
    "  * Inference latency\n",
    "  * Model configuration (layers, type, etc.)\n",
    "* **Example Toolkits**: Python + `openai` or `transformers` library.\n",
    "\n",
    "### üîå 2. **Estimate Energy Cost per Inference**\n",
    "\n",
    "* **Task**: Estimate the energy (kWh) consumed per prompt execution using:\n",
    "\n",
    "  * Known model compute cost (FLOPs or GPU time)\n",
    "  * Device power consumption specs (from server logs or public benchmarks)\n",
    "* **Formula**:\n",
    "\n",
    "  $$\n",
    "  \\text{kWh} = \\frac{\\text{FLOPs per inference} \\times \\text{Watt-hours per FLOP}}{3.6 \\times 10^6}\n",
    "  $$\n",
    "* **Tools**: Spreadsheet + Python script to convert FLOPs ‚Üí kWh.\n",
    "\n",
    "### üìä 3. **Label Real Prompts with Energy Use Estimates**\n",
    "\n",
    "* **Task**: Combine prompt features and estimated energy cost into a labeled dataset.\n",
    "* **Output**: A CSV or DataFrame with:\n",
    "\n",
    "  * Columns: `token_count`, `layers`, `flops_per_hour`, `avg_embedding_norm`, `bigram_likelihood`, `energy_kwh`\n",
    "  * Format: Ready for training the neural net.\n",
    "\n",
    "### üß™ 4. **Train and Evaluate the Model**\n",
    "\n",
    "* **Task**: Replace `X_train` and `y_train` with real data.\n",
    "* **Metrics to Report**:\n",
    "\n",
    "  * Mean Absolute Error (MAE)\n",
    "  * Mean Squared Error (MSE)\n",
    "  * R¬≤ Score\n",
    "* **Stretch Goal**: Plot training loss over epochs.\n",
    "\n",
    "### üßº 5. **Improve Data Quality (Optional)**\n",
    "\n",
    "* **Tasks**:\n",
    "\n",
    "  * Remove outliers (e.g. prompts with >2000 tokens)\n",
    "  * Normalize inputs again\n",
    "  * Add missing values imputation if needed\n",
    "  * Use embeddings from multiple prompt types for variety\n",
    "\n",
    "## üìÅ Example Dataset Output\n",
    "\n",
    "| layers | training\\_time | flops/hour | token\\_count | avg\\_embedding\\_norm | bigram\\_likelihood | energy\\_kwh |\n",
    "| ------ | -------------- | ---------- | ------------ | -------------------- | ------------------ | ----------- |\n",
    "| 12     | 48             | 2.0E+12    | 105          | 0.87                 | 0.031              | 148.2       |\n",
    "| 24     | 96             | 4.5E+12    | 330          | 0.92                 | 0.021              | 290.1       |\n",
    "| ...    | ...            | ...        | ...          | ...                  | ...                | ...         |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
